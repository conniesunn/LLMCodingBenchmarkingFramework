{
	"identifier": "problem_51",
	"description": "Implement the BLEU score metric for evaluating machine translation quality using unigram precision.",
	"function_prototype": {
		"function_name": "bleu_score",
		"parameters": [
			{"name": "reference", "type": "str"},
			{"name": "candidate", "type": "str"}
		],
		"return_values": [
			{"type": "float"}
		]
	},
	"correctness_test_suite": [
		{
			"input": {
				"reference": "the quick brown fox jumped over the lazy dog",
				"candidate": "quick fox jumped over the lazy dog"
			}, 
			"expected_output": [{
				"min": 0,
				"max": 1
			}]
		}
	],
	"tags": ["Natural Language Processing", "Machine Translation", "Metrics", "Medium"],
	"prompts": [
		{
			"prompt_id": "brief_prompt",			
			"prompt": "Implement the 'bleu_score' function to calculate the BLEU score metric for evaluating machine translation quality using unigram precision.",
			"genericize": false,
			"sample_inputs_outputs": [
				{
					"input": {
						"reference": "the quick brown fox jumped over the lazy dog",
						"candidate": "quick fox jumped over the lazy dog"
					},
					"expected_output": [{
						"min": 0,
						"max": 1
					}]
				}
			]
		},
		{
			"prompt_id": "detailed_prompt",
			"prompt": "Write a function named 'bleu_score' that takes two strings 'reference' and 'candidate'. The function should calculate the BLEU score metric, which is used for evaluating machine translation quality. For simplicity, use unigram precision. The score should be a float between 0 and 1.",
			"genericize": true,
			"sample_inputs_outputs": [
				{
					"input": {
						"reference": "the quick brown fox jumped over the lazy dog",
						"candidate": "quick fox jumped over the lazy dog"
					},
					"expected_output":[{
						"min": 0,
						"max": 1
					}]
				}
			]
		}
	]
}
