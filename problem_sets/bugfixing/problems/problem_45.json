{
    "identifier": "problem_45",
    "description": "Implement a safety layer on top of a basic text generator to prevent generating harmful or inappropriate content. Consider words like 'violence', 'hate' as harmful.",
    "function_prototype": {
        "function_name": "content_safety_filter",
        "parameters": [
            {
                "name": "text",
                "type": "str"
            }
        ],
        "return_values": [
            {
                "type": "str"
            }
        ]
    },
    "correctness_test_suite": [
        {
            "input": {
                "text": "Promote violence"
            },
            "expected_output": [{
                "output": "Content flagged"
            }]
        }
    ],
    "tags": ["Safety", "Content Filtering"],
    "prompts": [
        {
            "prompt_id": "brief_prompt",
            "prompt": "Implement a function 'content_safety_filter' that flags harmful or inappropriate content in a given text.",
            "genericize": false,
            "sample_inputs_outputs": [
                {
                    "input": {
                        "text": "Promote violence"
                    },
                    "expected_output": [{
                        "output": "Content flagged"
                    }]
                }
            ]
        },
        {
            "prompt_id": "detailed_prompt",
            "prompt": "Design a function named 'content_safety_filter' that examines a provided text for harmful or inappropriate content, such as words like 'violence' or 'hate'. If any such content is detected, the function should return 'Content flagged'.",
            "genericize": true,
            "sample_inputs_outputs": [
                {
                    "input": {
                        "text": "Promote violence"
                    },
                    "expected_output": [{
                        "output": "Content flagged"
                    }]
                }
            ]
        }
    ]
}
